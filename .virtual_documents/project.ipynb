# Initialize Otter
import otter
grader = otter.Notebook("hw5.ipynb")


import os

get_ipython().run_line_magic("matplotlib", " inline")
import string
import sys

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

from sklearn.compose import ColumnTransformer, make_column_transformer
from sklearn.dummy import DummyClassifier, DummyRegressor
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import (
    GridSearchCV,
    RandomizedSearchCV,
    cross_val_score,
    cross_validate,
    train_test_split,
)
from sklearn.pipeline import Pipeline, make_pipeline
from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler
from sklearn.svm import SVC, SVR
from sklearn.tree import DecisionTreeClassifier


random_state_ = 123


credit_df = pd.read_csv("data/credit_card.csv")
credit_df


from sklearn.model_selection import train_test_split

train_df, test_df = train_test_split(credit_df, test_size = 0.30, random_state=random_state_)
X_train, y_train = (
    train_df.drop(columns=["default.payment.next.month"]),
    train_df["default.payment.next.month"],
)
X_test, y_test = (
    test_df.drop(columns=["default.payment.next.month"]),
    test_df["default.payment.next.month"],
)


train_df.head()


train_df.info()


train_df.describe()


credit_plot = train_df.drop(columns = ["ID", "SEX", "EDUCATION", "MARRIAGE", "PAY_0", "PAY_2", "PAY_3", "PAY_4", "PAY_5", "PAY_6", "default.payment.next.month"])

for column in credit_plot:
    feat = column
    ax = train_df.groupby("default.payment.next.month")[feat].plot.hist(bins=50, alpha=0.5, legend=True)
    plt.xlabel(feat)
    plt.title("Histogram of " + feat)
    plt.show()


# Ignoring for the moment just to prioritize things


# confirming the values
train_df["EDUCATION"].unique()


train_df["PAY_0"].unique()


#Making the changes in training and testing set (note this is not breaking the golden rule)

for column in ["PAY_0", "PAY_2", "PAY_3", "PAY_4", "PAY_5", "PAY_6"]:
    for not_accepted in [-2, -1]:
        train_df.loc[train_df[column] == not_accepted, column] = 0
        test_df.loc[test_df[column] == not_accepted, column] = 0
    
for column in ["MARRIAGE"]:
    train_df.loc[train_df[column] == 0, column] = 3
    test_df.loc[test_df[column] == 0, column] = 3

for column in ["EDUCATION"]:
    for not_accepted in [0, 5, 6]:
        train_df.loc[ train_df[column] == not_accepted, column] = 4
        test_df.loc[ test_df[column] == not_accepted, column] = 4
    


# Inspired from https://ubc-cs.github.io/cpsc330/lectures/11_ensembles.html

# COLUMNS
numeric_features = ["BILL_AMT1", "BILL_AMT2", "BILL_AMT3", "BILL_AMT4", "BILL_AMT5", "BILL_AMT6",
                   "PAY_AMT1", "PAY_AMT2", "PAY_AMT3", "PAY_AMT4", "PAY_AMT5", "PAY_AMT6",
                   "LIMIT_BAL", "AGE"]
categorical_features = ["MARRIAGE"]
ordinal_features_education = ["EDUCATION"]
ordinal_features_pay = ["PAY_0", "PAY_2", "PAY_3", "PAY_4"]
# This will be explained below
ordinal_features_pay_56 = ["PAY_5", "PAY_6"]
# binary_features = ["SEX"]
drop_features = ["ID", "SEX"] # Dropping the feature to avoid gender based bias
target_column = "default.payment.next.month"

# drop_features = ["ID", "SEX", "PAY_0", "PAY_2", "PAY_3", "PAY_4"] # Debugging


# Ordinal Order

# (1=graduate school, 2=university, 3=high school, 4=others)
education_levels = [
     1, 2, 3, 4, 
]

# (0=pay duly, 1=payment delay for one month, 2=payment delay for two months, â€¦ 8=payment delay for eight months+)
pay_levels = [
    0, 1, 2, 3, 4, 5, 6, 7, 8,
]

# # For some reason in the entire dataset, pay 5 and 6 don't have a one month delay 
# # As this happens for all 21000 rows, we will assume this is not a fluke and there may be a bank policy on months 5 and 6.
pay_levels_56 = [
    0, 2, 3, 4, 5, 6, 7, 8,
]



assert set(education_levels) == set(train_df["EDUCATION"].unique())

assert set(pay_levels) == set(train_df["PAY_0"].unique())
assert set(pay_levels) == set(train_df["PAY_2"].unique())
assert set(pay_levels) == set(train_df["PAY_3"].unique())
assert set(pay_levels) == set(train_df["PAY_4"].unique())

assert set(pay_levels_56) == set(train_df["PAY_5"].unique()) 
assert set(pay_levels_56) == set(train_df["PAY_6"].unique())



# Transformers and Preprocessing
numeric_transformer = make_pipeline(
    StandardScaler()
)

ordinal_transformer_education = make_pipeline(
    OrdinalEncoder(categories=[education_levels], dtype=int, handle_unknown="use_encoded_value", unknown_value = 4)
)

ordinal_transformer_pay = make_pipeline(
    OrdinalEncoder(dtype=int, handle_unknown="use_encoded_value", unknown_value = -1)
)

ordinal_transformer_pay_56 = make_pipeline(
    OrdinalEncoder(dtype=int, handle_unknown="use_encoded_value", unknown_value = -1)
)

categorical_transformer = make_pipeline(
    OneHotEncoder(categories="auto", handle_unknown="ignore")
)

preprocessor = make_column_transformer(
    (numeric_transformer, numeric_features),
    (ordinal_transformer_education, ordinal_features_education),
    (ordinal_transformer_pay, ordinal_features_pay),
    (ordinal_transformer_pay_56, ordinal_features_pay_56),
    (categorical_transformer, categorical_features),
    ("drop", drop_features),
)


X_train = train_df.drop(columns=[target_column])
y_train = train_df[target_column]

X_test = test_df.drop(columns=[target_column])
y_test = test_df[target_column]


preprocessor.fit(X_train, y_train)
preprocessor.named_transformers_


X_train.head()


y_train.head()


results = {}


# We are choosing thisfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score

scoring_metric = ["accuracy",
    "precision",
] # TODO look at the model data


# From https://ubc-cs.github.io/cpsc330/lectures/05_preprocessing-pipelines.html#ordinal-encoding-occasionally-recommended
def mean_std_cross_val_scores(model, X_train, y_train, **kwargs):

    scores = cross_validate(model, X_train, y_train, **kwargs, error_score='raise')

    mean_scores = pd.DataFrame(scores).mean()
    std_scores = pd.DataFrame(scores).std()
    out_col = []

    for i in range(len(mean_scores)):
        out_col.append((f"%0.3f (+/- %0.3f)" % (mean_scores[i], std_scores[i])))

    return pd.Series(data=out_col, index=mean_scores.index)


# Dummy Baselike
dummy = DummyClassifier(strategy="stratified")
results["Dummy"] = mean_std_cross_val_scores(
    dummy, X_train, y_train, return_train_score=True, scoring=scoring_metric
)


preprocessor


pipe_dt = make_pipeline(preprocessor, DecisionTreeClassifier(random_state=random_state_))
results["Decision Tree"] = mean_std_cross_val_scores(
    pipe_dt, X_train, y_train, return_train_score=True, scoring=scoring_metric
)



pd.DataFrame(results).T


# linear model as first attempt
from sklearn.linear_model import LogisticRegression

# changing max_iter to deal with iterations reached limit
linear_model = LogisticRegression(max_iter = 1000)
linear_model.fit(X_train, y_train)
scores = cross_validate(linear_model, X_train, y_train, return_train_score = True)
pd.DataFrame(scores)


# tuning different values for the complexity hyperparameter
# reference: https://ubc-cs.github.io/cpsc330/lectures/07_linear-models.html?highlight=scores_dict#main-hyperparameter-of-logistic-regression
scores_dict = {
    "C": 10.0 ** np.arange(-4, 6, 1),
    "mean_train_scores": list(),
    "mean_cv_scores": list(),
}
for C in scores_dict["C"]:
    linear_model = LogisticRegression(max_iter = 1000, C = C)
    scores = cross_validate(linear_model, X_train, y_train, return_train_score=True)
    scores_dict["mean_train_scores"].append(scores["train_score"].mean())
    scores_dict["mean_cv_scores"].append(scores["test_score"].mean())

results_df = pd.DataFrame(scores_dict)
results_df


# report-cross validation scores along with standard deviation
results["Linear Model"] = mean_std_cross_val_scores(
    linear_model, X_train, y_train, return_train_score = True, scoring = scoring_metric
)
pd.DataFrame(results).T


# importing k-nn classifier
from sklearn.neighbors import KNeighborsClassifier

# setting number of neighbors to 3 since k = 1 will definitely overfit
knn_model = make_pipeline(preprocessor, KNeighborsClassifier(n_neighbors = 3))
results["K-NN"] = mean_std_cross_val_scores(
    knn_model, X_train, y_train, return_train_score = True, scoring = scoring_metric
)
pd.DataFrame(results).T


# importing random forest classifier
from sklearn.ensemble import RandomForestClassifier
random_forest_model = make_pipeline(
    preprocessor, RandomForestClassifier(random_state = random_state_, n_jobs = -1)
)
results["Random Forests"] = mean_std_cross_val_scores(
    random_forest_model, X_train, y_train, return_train_score = True, scoring = scoring_metric
)
pd.DataFrame(results).T


# importing catboost classifier
from catboost import CatBoostClassifier

catboost_model = make_pipeline(preprocessor, CatBoostClassifier(verbose = 0, random_state = random_state_))
results["CatBoost"] = mean_std_cross_val_scores(
    catboost_model, X_train, y_train, return_train_score = True, scoring = scoring_metric
)
pd.DataFrame(results).T


# ignoring for now to focus on other parts


# choosing the best hyperparameter k 
# reference: https://ubc-cs.github.io/cpsc330/lectures/04_kNNs-SVM-RBF.html#how-to-choose-n-neighbors
results_dict = {
    "n_neighbors": [],
    "mean_train_score": [],
    "mean_cv_score": [],
    "std_cv_score": [],
    "std_train_score": [],
}
param_grid = {"n_neighbors": np.arange(1, 50, 2)}

for k in param_grid["n_neighbors"]:
    knn = KNeighborsClassifier(n_neighbors=k)
    scores = cross_validate(knn, X_train, y_train, return_train_score=True)
    results_dict["n_neighbors"].append(k)

    results_dict["mean_cv_score"].append(np.mean(scores["test_score"]))
    results_dict["mean_train_score"].append(np.mean(scores["train_score"]))
    results_dict["std_cv_score"].append(scores["test_score"].std())
    results_dict["std_train_score"].append(scores["train_score"].std())

results_df = pd.DataFrame(results_dict)
results_df = results_df.set_index("n_neighbors")
results_df


results_df[["mean_train_score", "mean_cv_score"]].plot();


# choosing the best number of neighbors with the greatest cross-validation scores
best_n_neighbours = results_df.idxmax()["mean_cv_score"]
best_n_neighbours


# k-nn withh optimized hyperparameter
from sklearn.neighbors import KNeighborsClassifier

# passing the best k value to the model
knn_model_new = make_pipeline(preprocessor, KNeighborsClassifier(n_neighbors = best_n_neighbours))
results["K-NN Optimized"] = mean_std_cross_val_scores(
    knn_model_new, X_train, y_train, return_train_score=True, scoring = scoring_metric
)
pd.DataFrame(results).T


forest_pipe = Pipeline(
     [
        ("preprocess", preprocessor),
        ("rf", RandomForestClassifier(random_state = random_state_, n_jobs = -1)),
    ]
)


from sklearn.model_selection import GridSearchCV

# reference: https://ubc-cs.github.io/cpsc330/lectures/08_hyperparameter-optimization.html?highlight=gridsearchcv#the-syntax
param_grid = {
    "rf__n_estimators": [2, 5, 10, 15, 20, 25],
    "rf__max_depth": [1, 3, 5, 7, 10, 15, 20],
}

rforest_grid_search = GridSearchCV(
    forest_pipe, param_grid, cv = 5, n_jobs = -1, return_train_score=True
)


from sklearn import set_config
set_config(display = "diagram")


rforest_grid_search.fit(X_train, y_train)


rforest_grid_search.best_score_


rforest_grid_search.best_params_


random_forest_optimization_results = pd.DataFrame(rforest_grid_search.cv_results_)
random_forest_optimization_results.T


random_forest_optimization_results = (
    pd.DataFrame(rforest_grid_search.cv_results_).set_index("rank_test_score").sort_index()
)
display(random_forest_optimization_results.T)


pd.DataFrame(rforest_grid_search.cv_results_)[
    [
        "mean_test_score",
        "param_rf__n_estimators",
        "param_rf__max_depth",
        "mean_fit_time",
        "rank_test_score",
    ]
].set_index("rank_test_score").sort_index().T


random_forest_model_new = make_pipeline(
    preprocessor, RandomForestClassifier(n_estimators = 25, max_depth = 10, random_state = random_state_, n_jobs = -1)
)
results["Random Forests Optimized"] = mean_std_cross_val_scores(
    random_forest_model_new, X_train, y_train, return_train_score = True, scoring = scoring_metric
)
pd.DataFrame(results).T


catboost_pipe = Pipeline(
     [
        ("preprocess", preprocessor),
        ("catboost", CatBoostClassifier(verbose = 0, random_state = random_state_)),
    ]
)


# training time is too long - we decided to keep hyperparameter tuning to the minimum
catboost_param_grid = {
    "catboost__max_depth": [3, 4, 5],
    "catboost__n_estimators":[100, 200, 300]
}

catboost_grid_search = GridSearchCV(
    catboost_pipe, catboost_param_grid, cv = 5, n_jobs = -1, return_train_score=True
)


catboost_grid_search.fit(X_train, y_train)


catboost_grid_search.best_score_


catboost_grid_search.best_params_


catboost_optimization_results = pd.DataFrame(catboost_grid_search.cv_results_)
catboost_optimization_results.T


catboost_optimization_results = (
    pd.DataFrame(catboost_grid_search.cv_results_).set_index("rank_test_score").sort_index()
)
display(catboost_optimization_results.T)


pd.DataFrame(catboost_grid_search.cv_results_)[
    [
        "mean_test_score",
        "param_catboost__max_depth",
        "param_catboost__n_estimators",
        "mean_fit_time",
        "rank_test_score",
    ]
].set_index("rank_test_score").sort_index().T


catboost_model_new = make_pipeline(
    preprocessor, CatBoostClassifier(n_estimators = 300, max_depth = 3, verbose = 0, random_state = random_state_)
)
results["CatBoost Optimized"] = mean_std_cross_val_scores(
    catboost_model_new, X_train, y_train, return_train_score = True, scoring = scoring_metric
)
pd.DataFrame(results).T


import warnings

warnings.simplefilter(action="ignore", category=FutureWarning)


feature_names = numeric_features + categorical_features + ordinal_features_education + ordinal_features_pay + ordinal_features_pay_56 + drop_features


import eli5
random_forest_model_new.fit(X_train, y_train)
rf_eli5 = eli5.explain_weights(
    random_forest_model_new.named_steps["randomforestclassifier"], feature_names = feature_names, top = 22
)
rf_eli5


# reference: https://ubc-cs.github.io/cpsc330/lectures/12_feat-importances.html?highlight=drop_features#how-can-we-get-feature-importances-for-non-sklearn-models
# changing it to a dataframe
data = {
    "Importance": random_forest_model_new.named_steps["randomforestclassifier"].feature_importances_,
}
pd.DataFrame(data=data, index=feature_names,).sort_values(
    by="Importance", ascending=False
)[:10]


catboost_model_new.fit(X_train, y_train)
catboost_eli5 = eli5.explain_weights(
    catboost_model_new.named_steps["catboostclassifier"], feature_names = feature_names, top = 22
)
catboost_eli5


# changing it to a dataframe
data_1 = {
    "Importance": catboost_model_new.named_steps["catboostclassifier"].feature_importances_,
}
pd.DataFrame(data=data_1, index=feature_names,).sort_values(
    by="Importance", ascending=False
)[:10]


knn_model_new.fit(X_train, y_train)
knn_score = knn_model_new.score(X_test, y_test)


rf_score = random_forest_model_new.score(X_test, y_test)


cb_score = catboost_model_new.score(X_test, y_test)


data_score = {"real_test_score" : [knn_score, rf_score, cb_score]}
df_score = pd.DataFrame(data_score, index = ["K-NN Optimized", "Random Forest Optimized", "CatBoost Optimized"])
df_score


# displaying cross-validation scores for the optimized models
results_1 = pd.DataFrame(results).T
results_1.tail(n = 3)


X_train_enc = pd.DataFrame(
    data=preprocessor.transform(X_train),
    columns=feature_names,
    index=X_train.index,
)
X_train_enc.head()


X_test_enc = pd.DataFrame(
    data=preprocessor.transform(X_test),
    columns=feature_names,
    index=X_test.index,
)
X_test_enc.head()


import shap

rf_explainer = shap.TreeExplainer(random_forest_model_new.named_steps["randomforestclassifier"])
train_rf_shap_values = rf_explainer.shap_values(X_train_enc)

train_rf_shap_values


test_rf_shap_values = rf_explainer.shap_values(X_test_enc)
test_rf_shap_values


y_test_reset = y_test.reset_index(drop = True).astype(object)
y_test_reset


random_forest_model_new.named_steps["randomforestclassifier"].classes_


# pick two predictions
# y_test_reset.index.tolist()
zero_ind = y_test_reset[y_test_reset == 0].index.tolist()
one_ind = y_test_reset[y_test_reset == 1].index.tolist()

ex_zero_index = zero_ind[10]
ex_one_index = one_ind[10]


random_forest_model_new.named_steps["randomforestclassifier"].predict_proba(X_test_enc)[ex_zero_index]


rf_explainer


shap.force_plot(
    rf_explainer.expected_value[1],
    test_rf_shap_values[1][ex_zero_index, :],
    X_test_enc.iloc[ex_zero_index, :],
    matplotlib=True,
)


pd.DataFrame(
    test_rf_shap_values[1][ex_zero_index, :],
    index=feature_names,
    columns=["SHAP values"],
).sort_values('SHAP values', ascending=False)


random_forest_model_new.named_steps["randomforestclassifier"].predict_proba(X_test_enc)[ex_one_index]


shap.force_plot(
    rf_explainer.expected_value[1],
    test_rf_shap_values[1][ex_one_index, :],
    X_test_enc.iloc[ex_one_index, :],
    matplotlib=True,
)


pd.DataFrame(
    test_rf_shap_values[1][ex_one_index, :],
    index=feature_names,
    columns=["SHAP values"],
).sort_values('SHAP values', ascending=False)


df_score


rf_eli5


final_df = {"Machine Learning Model" : "RandomForest",
            "Test Score (Accuracy (%))" : cb_score * 100,
           "Most Important Feature": "EDUCATION",
            "Response Variable": "default.payment.next.month",
           "Optimized Parameters": "n_estimators = 300, max_depth = 3"}
pd.DataFrame(final_df, index = ["Final Results"])


from IPython.display import Image

Image("img/eva-well-done.png")



